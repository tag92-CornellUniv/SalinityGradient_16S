---
title: "Infer ASVs with DADA2"
output: html_document
toc: yes
toc_float: 
  collapsed: no
  smooth_scroll: yes
  toc_depth: 3
date: "2024-02-21"
editor_options: 
  chunk_output_type: console
---
#Before you start

##S et my seed
```{r set-seed}
#Setting our seed: makes the code reproducible; will pick the same random samples (the ones we used to generate the quality plots)

#Any number can be chosen 
set.seed(02262000)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center",
                      fig.path = "../figures/01_DADA2/")#send any figure output to this folder
```


# Goals of this file

1. use raw fastq files and generate the quality plots to assess quality of reads
2. filter and trim out bad sequences and bases from our raw sequencing files
3. Write out fastq files with high quality sequences
4. Evaluate the quality from our filter and trim
5. Generate error models
6. Identified ASVs for forward and reverse reads separately
7. Merge forward and reverse ASVs
8. Generate ASV count table (will be the otu table input for phyloseq)

Output that we need:

1.ASV Count table: otu_table
2. Taxonomy Table: tax_table
3. Sample Information: "sample_data" track the reads lots throughout the DADA2 workflow
# load libraries
```{r Load-libraries}

#library(devtools)

#install.packages("dada2")
library(dada2)

#install.packages("tidyverse")
library(tidyverse)

pacman::p_load(tidyverse, BiocManager, devtools, dada2, 
               phyloseq, patchwork, DT, iNEXT, vegan,
               install = FALSE)
```

# Load Data
```{r load-data}
# Set the raw fastq paths to the raw sequencing files
#Path to the fastq files
raw_fastqs_path <- "data/01_DADA2/01_rawgzipped_fastqs/"
raw_fastqs_path

#What files are in this path? Intuition Check
list.files(raw_fastqs_path) 


#how many files are there?
str(list.files(raw_fastqs_path))

#Create vector of forward reads
#forward_reads <- 
  
forward_reads <- list.files(raw_fastqs_path, pattern = "R1_001.fastq.gz", full.names = TRUE)
head(forward_reads)

#Create vector of reverse reads
reverse_reads <- list.files(raw_fastqs_path, pattern = "R2_001.fastq.gz", full.names = TRUE)
head(reverse_reads)

```

# Quality Plots
```{r raw-quality-plots}
# randomly select two samples for each dataset to evaluate
random_samples <- sample(1:length(reverse_reads), size = 2)
random_samples

#calculate and plot quality of htese two samples
forward_filteredQual_plot_12 <- plotQualityProfile(forward_reads[random_samples]) + 
  labs(title = "Forward Read: Raw Quality")

reverse_filteredQual_plot_12 <- plotQualityProfile(reverse_reads[random_samples]) + 
  labs(title = "Reverse Read: Raw Quality")

# Plot them together with patchwork
forward_filteredQual_plot_12 + reverse_filteredQual_plot_12

```

# Prepare a placeholder for filtered reads
```{r prep-filtered-sequences}
#vector for samples, extract sample name from files
samples <- sapply(strsplit(basename(forward_reads), "_"), `[`,1)
head(samples)

# Place filtered reads into filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"

# Create 2 variables: filtered_F, filtered_R
filtered_forward_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))
length(filtered_forward_reads)

filtered_reverse_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))
head(filtered_reverse_reads)
```



#Filter and Trim

Parameters of filter and trim depend on the DATA SET
Kozich et al. -> describe library prep and a protocol where they used 515F and 806R primers for full overlap
2 step illumina: you need to remove the primers
Does my dataset include the primers? Read the methods paper. 
Pair ended reads are better, so there are forward and reverse reads
```{r filter-and-trim}
#?filterAndTrim
#maxN= the number of n bases. Putting zeros removes Ns from the data. Every base matters.
#maxEE = expected errors. quality filtering threshold applied to expected errors. here, if there are less than 2 errors, its okay. However, more than two would result in the data being discarded. First value is for the forward read, the second is for the reverse read.
#trimLeft = remove first three basepairs 

#Assign a vector to filtered reads
#trim out poor bases, first 3 bp on F reads
#write out filtered fastq files
filtered_reads <- filterAndTrim(fwd = forward_reads, filt = filtered_forward_reads, rev = reverse_reads, filt.rev = filtered_reverse_reads, maxN = 0, maxEE = c(2,2), trimLeft= 3, truncQ = 2, rm.phix = TRUE, compress = TRUE) #multithread = TRUE)


```
# Trimmed quality plots
```{r filterTrim-quality-plots}
plotQualityProfile(filtered_forward_reads[random_samples]) + 
  labs(title = "Trimmed Forward Quality")

plotQualityProfile(filtered_reverse_reads[random_samples]) + 
  labs(title = "Trimmed Reverse Quality")


```

# Aggregated Trimmed Plots
```{r aggregated-trimmed-plots}
#aggregate all QC plots
plotQualityProfile(filtered_forward_reads, aggregate = TRUE) +
plotQualityProfile(filtered_reverse_reads, aggregate = TRUE)

```

# Stats on read outpur from "filterandTrim"
```{r filterTrim-stats}
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)


filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = (median(reads.out)/median(reads.in)))

```

# error Modelling

**NOTE: Run separately on each dataset
```{r learn-errors}
#run on the forward reads
error_forward_reads <- 
  learnErrors(filtered_forward_reads) # multithread = TRUE

#run on the reverse reads
error_reverse_reads <- 
  learnErrors(filtered_reverse_reads) # multithread = TRUE

plotErrors(error_forward_reads, nominalQ = TRUE) + 
  labs(title = "Forward Read Error Model")

plotErrors(error_reverse_reads, nominalQ = TRUE) + 
  labs(title = "Reverse Read Error Model")
```

#Infer ASVs

Note that this is happening separately on the forward and reverse reads. This is unique to DADA2.
```{r infer-ASVs}
#Infer forward reads
dada_forward <- dada(filtered_forward_reads, err = error_forward_reads) #multithread = TRUE

#Infer reverse reads
dada_reverse <- dada(filtered_reverse_reads, err = error_reverse_reads) #multithread = TRUE

```

# Merge Forward and Reverse ASVs
```{r merge-ASVs}
#merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads,
                          dada_reverse, filtered_reverse_reads,
                          verbose = TRUE)

#Evaluate the output 
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)
```

# Generate ASV Count Table
```{r}
#Create the ASV Count table
raw_ASV_table <- makeSequenceTable(merged_ASVs)

#Write out the file to data/01_DADA2 

```

# Create Raw ASV Count Table 
```{r generate-ASV-table, fig.width=3.5, fig.height=3}
# Create the ASV Count Table 
raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2


# Check the type and dimensions of the data
dim(raw_ASV_table)
class(raw_ASV_table)
typeof(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length")


###################################################
###################################################
# TRIM THE ASVS
# Let's trim the ASVs to only be the right size, which is 249.
# 249 originates from our expected amplicon of 252 - 3bp in the forward read due to low quality.

# We will allow for a few 
raw_ASV_table_trimmed <- raw_ASV_table[,nchar(colnames(raw_ASV_table)) %in% 248:250]

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed)/sum(raw_ASV_table)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")
# Note the peak at 249 is ABOVE 3000

# Let's zoom in on the plot 
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length") + 
  scale_y_continuous(limits = c(0, 500))
```


# Remove Chimeras

Sometimes chimeras arise in our workflow. 

**Chimeric sequences** are artificial sequences formed by the combination of two or more distinct biological sequences. These chimeric sequences can arise during the polymerase chain reaction (PCR) amplification step of the 16S rRNA gene, where fragments from different templates can be erroneously joined together.

Chimera removal is an essential step in the analysis of 16S sequencing data to improve the accuracy of downstream analyses, such as taxonomic assignment and diversity assessment. It helps to avoid the inclusion of misleading or spurious sequences that could lead to incorrect biological interpretations.

```{r rm_chimeras, fig.width=3.5, fig.height=3}
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table <- removeBimeraDenovo(raw_ASV_table_trimmed, 
                                           method="consensus", 
                                           multithread=TRUE, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table)/sum(raw_ASV_table_trimmed)
sum(noChimeras_ASV_table)/sum(raw_ASV_table)

# Plot it 
data.frame(Seq_Length_NoChim = nchar(getSequences(noChimeras_ASV_table))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")

```

# Track the read counts
Here, we will look at the number of reads that were lost in the filtering, denoising, merging, and chimera removal. 
```{r track_reads, fig.width=6, fig.height=4}
# A little function to identify number seqs 
getN <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track <- cbind(filtered_reads, 
               sapply(dada_forward, getN),
               sapply(dada_reverse, getN),
               sapply(merged_ASVs, getN),
               rowSums(noChimeras_ASV_table))

head(track)

# Update column names to be more informative (most are missing at the moment!)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- samples

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df <- 
  track %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

# Visualize it in table format 
DT::datatable(track_counts_df)

# Plot it!
track_counts_df %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```

# Assign Taxonomy 

Here, we will use the silva database version 138!
```{r assign-tax}
# Classify the ASVs against a reference set using the RDP Naive Bayesian Classifier described by Wang et al., (2007) in AEM
taxa_train <- 
  assignTaxonomy(noChimeras_ASV_table, 
                 "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", 
                 multithread=TRUE)

# Add the genus/species information 
taxa_addSpecies <- 
  addSpecies(taxa_train, 
             "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)
```

# Prepare the data for export! 

## 1. ASV Table 

Below, we will prepare the following: 

1. Two ASV Count tables: 
      a. With ASV seqs: ASV headers include the *entire* ASV sequence ~250bps.
      b. with ASV names: This includes re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below.  
2. `ASV_fastas`: A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  

### Finalize ASV Count Tables 
```{r prepare-ASVcount-table}
########### 2. COUNT TABLE ###############
############## Modify the ASV names and then save a fasta file!  ############## 
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(noChimeras_ASV_table)
asv_seqs[1:5]

# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(noChimeras_ASV_table)[2], mode = "character")
asv_headers[1:5]

# loop through vector and fill it in with ASV names 
for (i in 1:dim(noChimeras_ASV_table)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
asv_headers[1:5]

##### Rename ASVs in table then write out our ASV fasta file! 
#View(noChimeras_ASV_table)
asv_tab <- t(noChimeras_ASV_table)
#View(asv_tab)

## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)
#View(asv_tab)
```


## 2. Taxonomy Table 
```{r prepare-tax-table}
# Inspect the taxonomy table
#View(taxa_addSpecies)

##### Prepare tax table 
# Add the ASV sequences from the rownames to a column 
new_tax_tab <- 
  taxa_addSpecies%>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(noChimeras_ASV_table))

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)
head(new_tax_tab)

### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

head(asv_tax)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))
```

# Write `01_DADA2` files

1. `ASV_counts.tsv`: ASV count table that has ASV names that are re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below. This will also be saved as `data/01_DADA2/ASV_counts.RData`.
2. `ASV_counts_withSeqNames.tsv`: This is generated with the data object in this file known as `noChimeras_ASV_table`. ASV headers include the *entire* ASV sequence ~250bps.  In addition, we will save this as a .RData object as `data/01_DADA2/noChimeras_ASV_table.RData` as we will use this data in `analysis/02_Taxonomic_Assignment.Rmd` to assign the taxonomy from the sequence headers.  
3. `ASVs.fasta`: A fasta file output of the ASV names from `ASV_counts.tsv` and the sequences from the ASVs in `ASV_counts_withSeqNames.tsv`. A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  
4. We will also make a copy of `ASVs.fasta` in `data/02_TaxAss_FreshTrain/` to be used for the taxonomy classification in the next step in the workflow.  
5. Write out the taxonomy table
6. `track_read_counts.RData`: To track how many reads we lost throughout our workflow that could be used and plotted later. We will add this to the metadata in `analysis/02_Taxonomic_Assignment.Rmd`.   

```{r save-files}
# FIRST, we will save our output as regular files, which will be useful later on. 
# Save to regular .tsv file 
# Write BOTH the modified and unmodified ASV tables to a file!
# Write count table with ASV numbered names (e.g. ASV_1, ASV_2, etc)
write.table(asv_tab, "data/01_DADA2/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write count table with ASV sequence names
write.table(noChimeras_ASV_table, "data/01_DADA2/ASV_counts_withSeqNames.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# Save to a file!
write(asv_fasta, "data/01_DADA2/ASVs.fasta")


# SECOND, let's save the taxonomy tables 
# Write the table 
write.table(asv_tax, "data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)


# THIRD, let's save to a RData object 
# Each of these files will be used in the analysis/02_Taxonomic_Assignment
# RData objects are for easy loading :) 
save(noChimeras_ASV_table, file = "data/01_DADA2/noChimeras_ASV_table.RData")
save(asv_tab, file = "data/01_DADA2/ASV_counts.RData")
# And save the track_counts_df a R object, which we will merge with metadata information in the next step of the analysis in nalysis/02_Taxonomic_Assignment. 
save(track_counts_df, file = "data/01_DADA2/track_read_counts.RData")
```

# Session information
```{r session-info}
devtools::session_info()


```

